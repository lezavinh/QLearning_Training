# Author: Vinh Le Za

#! /usr/bin/env python

import rospy
from time import time
from time import sleep
from datetime import datetime
from src.turtlebot3_dqn.environment_stage_4 import Env
import matplotlib.pyplot as plt
from Lidar import *

import sys
DATA_PATH = '/home/le/aev_ws/src/turtlebot3_machine_learning/turtlebot3_dqn/Data'
MODULES_PATH = '/home/le/autonomous_ws/src/agv_qlearning/scripts'
sys.path.insert(0, MODULES_PATH)

# Episode parameters
MAX_EPISODES = 400
MAX_STEPS_PER_EPISODE = 500
MIN_TIME_BETWEEN_ACTIONS = 0.0

# Learning parameters
ALPHA = 0.5
GAMMA = 0.9

T_INIT = 25
T_GRAD = 0.95
T_MIN = 0.001

EPSILON_INIT = 0.9
EPSILON_GRAD = 0.96
EPSILON_MIN = 0.05

# 1 - Softmax , 2 - Epsilon greedy
EXPLORATION_FUNCTION = 1
# Initial position
X_INIT = -0.4
Y_INIT = -0.4
THETA_INIT = 45.0

RANDOM_INIT_POS = False
LOG_FILE_DIR = DATA_PATH + '/Log_learning'
# Q table source file
Q_SOURCE_DIR = ''

def initLearning(env):
    global actions, state_space, Q_table
    actions = env.createActions()
    state_space = env.createStateSpace()
    if Q_SOURCE_DIR != '':
        Q_table = env.readQTable(Q_SOURCE_DIR+'/Qtable.csv')
    else:
        Q_table = env.createQTable(len(state_space),len(actions))
    print('Initial Q-table:')
    print(Q_table)
def initParams():
    global T, EPSILON, alpha, gamma
    global ep_steps, ep_reward, episode, steps_per_episode, reward_per_episode
    global robot_in_pos, first_action_taken
 
    # Learning parameters
    T = T_INIT
    EPSILON = EPSILON_INIT
    alpha = ALPHA
    gamma = GAMMA

    # Episodes, steps, rewards
    ep_steps = 0
    ep_reward = 0
    episode = 1
    crash = 0

    # initial position
    robot_in_pos = False
    first_action_taken = False

    # Date
    now_start = datetime.now()
    dt_string_start = now_start.strftime("%d/%m/%Y %H:%M:%S")
    
    text = '\r\n' + 'SIMULATION START ==> ' + dt_string_start + '\r\n\r\n'
    print(text)
if __name__ == '__main__':
    try:
        global actions, state_space, Q_table
        global T, EPSILON, alpha, gamma
        global ep_steps, ep_reward, episode, steps_per_episode, reward_per_episode
        global robot_in_pos, first_action_taken

        rospy.init_node('learning_node', anonymous = False)
        rate = rospy.Rate(10)

        #setPosPub = rospy.Publisher('/gazebo/set_model_state', ModelState, queue_size = 10)
        #velPub = rospy.Publisher('/cmd_vel', Twist, queue_size = 10)

        env = Env()
        initLearning()
        initParams()
        #sleep(5)

        # main loop
        while not rospy.is_shutdown():
            msgScan = rospy.wait_for_message('/scan', LaserScan)
            # Secure the minimum time interval between 2 actions
            step_time = (rospy.Time.now() - t_step).to_sec()
            if step_time > MIN_TIME_BETWEEN_ACTIONS:
                t_step = rospy.Time.now()
                if step_time > 2:
                    text = '\r\nTOO BIG STEP TIME: %.2f s' % step_time
                    print(text)
                    #log_sim_info.write(text+'\r\n')

                # End of Learning
                if episode > MAX_EPISODES:

                    env.saveQTable(LOG_FILE_DIR+'/Qtable.csv', Q_table)

                    rospy.signal_shutdown('End of learning')
                else:
                    ep_time = (rospy.Time.now() - t_ep).to_sec()
                    # End of en Episode
                    if crash or ep_steps >= MAX_STEPS_PER_EPISODE:
                        robotStop(velPub)
                        ep_steps = 0
                        ep_reward = 0
                        crash = 0
                        robot_in_pos = False
                        first_action_taken = False
                        if T > T_MIN:
                            T = T_GRAD * T
                        if EPSILON > EPSILON_MIN:
                            EPSILON = EPSILON_GRAD * EPSILON
                        episode = episode + 1
                    else:
                        ep_steps = ep_steps + 1
                        # Initial position
                        if not robot_in_pos:
                            robotStop(velPub)
                            ep_steps = ep_steps - 1
                            first_action_taken = False
                            # init pos
                            if RANDOM_INIT_POS:
                                ( x_init , y_init , theta_init ) = robotSetRandomPos(setPosPub)
                            else:
                                ( x_init , y_init , theta_init ) = robotSetPos(setPosPub, X_INIT, Y_INIT, THETA_INIT)

                            odomMsg = rospy.wait_for_message('/odom', Odometry)
                            ( x , y ) = getPosition(odomMsg)
                            theta = degrees(getRotation(odomMsg))
                            # check init pos
                            if abs(x-x_init) < 0.01 and abs(y-y_init) < 0.01 and abs(theta-theta_init) < 1:
                                robot_in_pos = True
                                #sleep(2)
                            else:
                                robot_in_pos = False
                        # First action
                        elif not first_action_taken:
                            ( lidar, angles ) = lidarScan(msgScan)
                            ( state_ind, x1, x2 ,x3 ,x4 ) = scanDiscretization(state_space, lidar)
                            crash = checkCrash(lidar)

                            if EXPLORATION_FUNCTION == 1 :
                                ( action, status_strat ) = env.softMaxSelection(Q_table, state_ind, actions, T)
                            else:
                                ( action, status_strat ) = env.epsiloGreedyExploration(Q_table, state_ind, actions, T)

                            status_rda = robotDoAction(velPub, action)

                            prev_lidar = lidar
                            prev_action = action
                            prev_state_ind = state_ind

                            first_action_taken = True

                            if not (status_strat == 'softMaxSelection => OK' or status_strat == 'epsiloGreedyExploration => OK'):
                                print('\r\n', status_strat, '\r\n')
                                #log_sim_info.write('\r\n'+status_strat+'\r\n')

                            if not status_rda == 'robotDoAction => OK':
                                print('\r\n', status_rda, '\r\n')
                                #log_sim_info.write('\r\n'+status_rda+'\r\n')

                        # Rest of the algorithm
                        else:
                            ( lidar, angles ) = lidarScan(msgScan)
                            ( state_ind, x1, x2 ,x3 ,x4 ) = scanDiscretization(state_space, lidar)
                            crash = checkCrash(lidar)

                            ( reward, terminal_state ) = env.getReward(action, prev_action, lidar, prev_lidar, crash)

                            ( Q_table, status_uqt ) = env.updateQTable(Q_table, prev_state_ind, action, reward, state_ind, alpha, gamma)

                            if EXPLORATION_FUNCTION == 1:
                                ( action, status_strat ) = softMaxSelection(Q_table, state_ind, actions, T)
                            else:
                                ( action, status_strat ) = epsiloGreedyExploration(Q_table, state_ind, actions, T)

                            status_rda = robotDoAction(velPub, action)

                            if not status_uqt == 'updateQTable => OK':
                                print('\r\n', status_uqt, '\r\n')
                                #log_sim_info.write('\r\n'+status_uqt+'\r\n')
                            if not (status_strat == 'softMaxSelection => OK' or status_strat == 'epsiloGreedyExploration => OK'):
                                print('\r\n', status_strat, '\r\n')
                                #log_sim_info.write('\r\n'+status_strat+'\r\n')
                            if not status_rda == 'robotDoAction => OK':
                                print('\r\n', status_rda, '\r\n')
                                #log_sim_info.write('\r\n'+status_rda+'\r\n')

                            ep_reward = ep_reward + reward
                            ep_reward_arr = np.append(ep_reward_arr, reward)
                            prev_lidar = lidar
                            prev_action = action
                            prev_state_ind = state_ind

    except rospy.ROSInterruptException:
        robotStop(velPub)
        print('Simulation terminated!')
        pass
